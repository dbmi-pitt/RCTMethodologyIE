{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WEKX_s5cw_R"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ],
      "id": "2WEKX_s5cw_R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unique-mouth"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "import pandas as pd\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "from sklearn.model_selection import KFold, GroupKFold, train_test_split, GroupShuffleSplit\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report,  accuracy_score, precision_score, recall_score, f1_score"
      ],
      "id": "unique-mouth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timely-separate"
      },
      "outputs": [],
      "source": [
        "train_file = open(\"TRAININGDATA_similarity_sampled_sentences_BERT_10112022.txt\",\"r+\")\n",
        "test_file = open(\"TESTINGDATA_07232022_BERT.txt\",\"r+\")\n",
        "\n",
        "train_data = []\n",
        "#unique_information_items_training_set = []\n",
        "train_data_lines = train_file.readlines()\n",
        "for line in train_data_lines:\n",
        "    if line !=\"\":\n",
        "        line = line.replace(\"\\n\",\"\")\n",
        "        annotation = line.split(\"|\")\n",
        "        train_data.append(annotation)\n",
        "    #unique_information_items_training_set.append(annotation[2])\n",
        "print (\"Number of instances in training set: \", len(train_data))\n",
        "train_data = pd.DataFrame(\n",
        "    train_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
        "display(train_data.head(10))\n",
        "\n",
        "test_data = []\n",
        "#unique_information_items_test_set = []\n",
        "test_data_lines = test_file.readlines()\n",
        "for line in test_data_lines:\n",
        "    if line != \"\":\n",
        "        line = line.replace(\"\\n\",\"\")\n",
        "        annotation = line.split(\"|\")\n",
        "        test_data.append(annotation)\n",
        "    #unique_information_items_test_set.append(annotation[2])\n",
        "print (\"Number of instances in testing set: \", len(test_data))\n",
        "test_data = pd.DataFrame(\n",
        "    test_data, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
        "display(test_data.head(10))\n",
        "\n"
      ],
      "id": "timely-separate"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCeuSkBod1F1"
      },
      "outputs": [],
      "source": [
        "unique_train_labels = set(list(train_data[\"labels\"]))\n",
        "print (len(unique_train_labels))\n",
        "unique_test_labels = set(list(test_data[\"labels\"]))\n",
        "print (len(unique_test_labels))\n",
        "all_unique_labels = unique_train_labels.union(unique_test_labels)\n",
        "print (len(all_unique_labels))\n",
        "all_unique_labels = list(all_unique_labels)\n",
        "print (all_unique_labels)"
      ],
      "id": "hCeuSkBod1F1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "devoted-reminder",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7)\n",
        "# split = splitter.split(train_data, groups=train_data['sentence_id'])\n",
        "# train_inds, eval_inds = next(split)\n",
        "\n",
        "# train_df = train_data.iloc[train_inds]\n",
        "# eval_df = train_data.iloc[eval_inds]\n",
        "train_df = train_data\n",
        "print (\"Training set: \", len(train_df))\n",
        "train_df = train_df[[\"sentence_id\",\"words\",\"labels\"]]\n",
        "display(train_df.head(3))\n",
        "# print (\"Evaluation set: \", len(eval_df))\n",
        "# eval_df = eval_df[[\"sentence_id\",\"words\",\"labels\"]]\n",
        "# display(eval_df.head(3))"
      ],
      "id": "devoted-reminder"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "israeli-relaxation"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# Configure the model\n",
        "model_args = NERArgs()\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.evaluate_during_training = False\n",
        "model_args.manual_seed = 4\n",
        "model_args.use_multiprocessing = True\n",
        "model_args.train_batch_size = 4\n",
        "model_args.eval_batch_size = 4\n",
        "model_args.use_auth_token=True\n",
        "model_args.num_train_epochs = 20\n",
        "model_args.learning_rate = 3e-5\n",
        "model_args.save_steps= -1\n",
        "model_args.save_model_every_epoch = False\n",
        "\n",
        "model = NERModel(\"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", \n",
        "                 args=model_args,\n",
        "                 labels=all_unique_labels)\n"
      ],
      "id": "israeli-relaxation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "secret-tracy"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.train_model(train_df)\n",
        "    "
      ],
      "id": "secret-tracy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WAQWnzwiIMU"
      },
      "outputs": [],
      "source": [
        "result, model_outputs, wrong_preds = model.eval_model(train_df)\n",
        "print (\"Results: \", result)"
      ],
      "id": "_WAQWnzwiIMU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8fm7PCKiPgP"
      },
      "outputs": [],
      "source": [
        "test_result, test_model_outputs, test_preds_labels = model.eval_model(test_data)\n",
        "print (len(test_preds_labels))"
      ],
      "id": "a8fm7PCKiPgP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaa-HuN8jxK3"
      },
      "outputs": [],
      "source": [
        "display(test_data.head(2))\n",
        "test_true_labels = test_data.groupby('sentence_id')['labels'].apply(list)\n",
        "print(type(test_true_labels.tolist()))\n",
        "print (len(test_true_labels))\n",
        "print (len(test_true_labels[1497]))\n",
        "print (len(test_preds_labels[1497]))"
      ],
      "id": "uaa-HuN8jxK3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "challenging-pension"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_true_labels_lst = []\n",
        "test_pred_labels_lst = []\n",
        "count = 0 \n",
        "for i in range (0,len(test_true_labels)):\n",
        "    sentence_true_label = test_true_labels[i]\n",
        "    sentence_predict_label = test_preds_labels[i]\n",
        "    if (len(sentence_true_label) == len(sentence_predict_label)):\n",
        "      count +=1\n",
        "      for item in sentence_true_label:\n",
        "          test_true_labels_lst.append(item)\n",
        "      for item in sentence_predict_label:\n",
        "          test_pred_labels_lst.append(item)\n",
        "print (count)          \n",
        "print (\"True labels in eval set: \", len(test_true_labels_lst))\n",
        "print (\"Predicted labels in eval set: \", len(test_pred_labels_lst)) \n",
        "unique_labels = list(set(test_true_labels_lst))\n",
        "unique_labels.remove('O')\n",
        "\n",
        "print ('Accuracy:', accuracy_score(test_true_labels_lst,test_true_labels_lst))\n",
        "print ('Precision:', precision_score(test_true_labels_lst,test_pred_labels_lst, average = \"micro\", labels = unique_labels))\n",
        "print ('Recall:', recall_score(test_true_labels_lst,test_pred_labels_lst,average = \"micro\", labels = unique_labels))\n",
        "print ('F1 score:', f1_score(test_true_labels_lst,test_pred_labels_lst, average = \"micro\", labels = unique_labels))\n",
        "print (classification_report(test_true_labels_lst,test_pred_labels_lst, labels=unique_labels ))\n"
      ],
      "id": "challenging-pension"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pleased-clock"
      },
      "outputs": [],
      "source": [
        "report = classification_report(test_true_labels_lst,test_pred_labels_lst, labels=unique_labels, output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "file_name = \"test_set_lr55_epoch20_similarity_sampling_BERT_10222022.csv\"\n",
        "report_df.to_csv(file_name)\n",
        "\n",
        "textfile = open(\"outputs_lr55_epoch20_similarity_sampling_BERT_10222022.txt\", \"w\")\n",
        "for element in test_preds_labels:\n",
        "    textfile.write(str(element) + \"\\n\")\n",
        "textfile.close()\n"
      ],
      "id": "pleased-clock"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alien-tournament"
      },
      "outputs": [],
      "source": [
        "textfile_true = open(\"true_labels_test_BERT_10222022.txt\", \"w\")\n",
        "for element in test_true_labels:\n",
        "    textfile_true.write(str(element) + \"\\n\")\n",
        "textfile_true.close()"
      ],
      "id": "alien-tournament"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "considered-prediction"
      },
      "outputs": [],
      "source": [
        "#Document level\n",
        "\n",
        "test_data['sentence'] = test_data[['sentence_id','words','labels']].groupby(['sentence_id'])['words'].transform(lambda x: ' '.join(x))\n",
        "# let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
        "test_data['word_labels'] = test_data[['sentence_id','words','labels']].groupby(['sentence_id'])['labels'].transform(lambda x: ','.join(x))\n",
        "\n",
        "test_data = test_data[[\"sentence_id\", \"word_labels\", \"sentence\"]]\n",
        "test_data = test_data.drop_duplicates(keep = \"first\")\n",
        "print (len(test_data))\n",
        "display(test_data.head(10))"
      ],
      "id": "considered-prediction"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quantitative-incident"
      },
      "outputs": [],
      "source": [
        "test_data_test = test_data\n",
        "test_data_test[\"pred_labels\"] = \"\"\n",
        "for index, row in test_data_test.iterrows():\n",
        "    sentence = row[\"sentence\"].split(\" \")\n",
        "    sentence_str = \"\"\n",
        "    for word in sentence:\n",
        "        sentence_str = sentence_str + word + \" \"\n",
        "#     print (sentence_str)\n",
        "    sentence_labels = row[\"word_labels\"]\n",
        "    sentence_labels_lst = sentence_labels.split(\",\")\n",
        "#     print (\"True: \", (sentence_labels_lst), len(sentence_labels_lst))\n",
        "    predictions, _ = model.predict([sentence_str])\n",
        "    \n",
        "    sentence_prediction = []\n",
        "    predictions_list = predictions[0]\n",
        "    for token in predictions_list:\n",
        "        for key, value in token.items():\n",
        "            prediction_token = value\n",
        "            sentence_prediction.append(prediction_token)\n",
        "    row[\"pred_labels\"] = sentence_prediction    \n",
        "display(test_data_test)\n",
        "    "
      ],
      "id": "quantitative-incident"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "regular-mediterranean"
      },
      "outputs": [],
      "source": [
        "test_data_test.to_csv(\"prediction_TEST_SET_lr55_epoch20_random_similarity_sampling_BERT_10222022.csv\")"
      ],
      "id": "regular-mediterranean"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "corporate-bunny"
      },
      "outputs": [],
      "source": [
        "prediction_file = \"USERSTUDY_PREDICTIONS_SENTENCES_10302022.txt\"\n",
        "f = open(prediction_file, \"r\")\n",
        "prediction_sentences = f.readlines()\n",
        "print (len(prediction_sentences))\n",
        "sentences_list = []\n",
        "predictions_list = []\n",
        "\n",
        "for sentence in prediction_sentences:\n",
        "  sentence_prediction = []\n",
        "  predictions, _ = model.predict([sentence])\n",
        "  prediction_sentences = predictions[0]\n",
        "  for token in prediction_sentences:\n",
        "    for key, value in token.items():\n",
        "      prediction_token = value\n",
        "      sentence_prediction.append(prediction_token)\n",
        "  print (sentence, sentence_prediction)\n",
        "  sentences_list.append(sentence)\n",
        "  predictions_list.append(sentence_prediction)"
      ],
      "id": "corporate-bunny"
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df = pd.DataFrame(list(zip(sentences_list, predictions_list)))\n",
        "prediction_df.to_csv(\"prediction_df.csv\")"
      ],
      "metadata": {
        "id": "WfKZ5wfhlIg0"
      },
      "id": "WfKZ5wfhlIg0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}